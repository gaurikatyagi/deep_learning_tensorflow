{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In statistics, logistic regression, or logit regression, or logit model is a regression model where the dependent variable (DV) is categorical.\n",
    "\n",
    "Logistic regression is used to describe data and to explain the relationship between one dependent binary variable and one or more nominal, ordinal, interval or ratio-level independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "#Import MNIST\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Setting up tensforflow parameters\n",
    "learning_rate =  0.03 \n",
    "training_epochs = 25\n",
    "batch_size = 100\n",
    "\n",
    "#Building a deeper network\n",
    "n_hidden_1 = 256 #number of nodes in hidden layer 1\n",
    "n_hidden_2 = 256 #number of features in hidden layer 2\n",
    "\n",
    "n_inputs = 784 #MNIST data 28*28=784 for each image\n",
    "n_classes = 10 #Output can be 1 of the 10 numbers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#TF graph inputs\n",
    "\n",
    "#mnist images are of shape 28*28 = 784\n",
    "#We do not know the number of images/ they will be different for train and test thus None, 784\n",
    "x = tf.placeholder(tf.float32, [None, n_inputs])\n",
    "\n",
    "#y, i.e. the result can be any number from 0 to 9 in onehot encoded format\n",
    "y = tf.placeholder(tf.float32, [None, n_classes]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Defining weights and bias\n",
    "W = {\"w1\": tf.Variable(tf.random_normal([n_inputs, n_hidden_1])),\n",
    "     \"w2\": tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "     \"w_\": tf.Variable(tf.random_normal([n_hidden_2, n_classes]))\n",
    "    }\n",
    "\n",
    "b = {\"b1\": tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "     \"b2\": tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "     \"b_\": tf.Variable(tf.random_normal([n_classes]))\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Model construction\n",
    "def multilayer_perceptron(x, ws, bs):\n",
    "    #Hidden layer1 with RELU activation\n",
    "    layer_1 = tf.add(tf.matmul(x,ws[\"w1\"]), bs[\"b1\"])\n",
    "    output_layer_1 = tf.nn.relu(layer_1)\n",
    "    #Hidden layer2 with RELU activation\n",
    "    layer_2 = tf.add(tf.matmul(output_layer_1,ws[\"w2\"]), bs[\"b2\"])\n",
    "    output_layer_2 = tf.nn.relu(layer_2)\n",
    "    #Final output layer \n",
    "    output = tf.add(tf.matmul(output_layer_2, ws[\"w_\"]), bs[\"b_\"])\n",
    "    return output\n",
    "\n",
    "#Model call\n",
    "pred = multilayer_perceptron(x, W, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Now, every time this model is rerun to train, it will work towards reducing the loss and error\n",
    "#Loss reduction will be based on cross entropy 1/(1+e**-x)\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits= pred, labels= y))\n",
    "\n",
    "# AdamOptimizer training\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 Cost: 40.5793464544\n",
      "Epoch: 06 Cost: 0.798111425259\n",
      "Epoch: 11 Cost: 0.56965863702\n",
      "Epoch: 16 Cost: 0.475053905411\n",
      "Epoch: 21 Cost: 0.40898494012\n",
      "Training finished\n",
      "Accuracy: 0.8753\n"
     ]
    }
   ],
   "source": [
    "##Initialize all variables and launch session\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    #Training steps\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        \n",
    "        #Training will be in batches now\n",
    "        for i in range(total_batch):\n",
    "            #pick up one batch at a time\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            _, c = sess.run([optimizer, cost], feed_dict = {x: batch_xs,\n",
    "                                                  y: batch_ys})\n",
    "            new_cost = c/total_batch\n",
    "            #computing intermediary loss\n",
    "            avg_cost = avg_cost + new_cost\n",
    "        \n",
    "        if epoch%5 ==0:\n",
    "            print \"Epoch:\", \"%02d\" % (epoch+1), \"Cost:\", avg_cost\n",
    "            \n",
    "    print \"Training finished\"  \n",
    "    \n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    # Calculate accuracy for 3000 examples\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print \"Accuracy:\", accuracy.eval({x: mnist.test.images, y: mnist.test.labels})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This 2 hidden layer model performs better than the regular logistic regression."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
