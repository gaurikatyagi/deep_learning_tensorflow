{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Convolution is the concent of a sliding window function which moves a amount defined by the stride.\n",
    "\n",
    "Now you know what convolutions are. But what about CNNs? CNNs are basically just several layers of convolutions with nonlinear activation functions like ReLU or tanh applied to the results. In a traditional feedforward neural network we connect each input neuron to each output neuron in the next layer. That’s also called a fully connected layer, or affine layer. In CNNs we don’t do that. Instead, we use convolutions over the input layer to compute the output. This results in local connections, where each region of the input is connected to a neuron in the output. \n",
    "\n",
    "Pooling refers to reducing the output of all the layers 1 level of the network which is passed as 1 input again to the next iteration layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from IPython.display import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See illustration at http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/\n",
    "\n",
    "Also, Convolutional neural networks (CNN) are designed to recognize images. It has convolutions inside, which see the edges of an object recognized on the image. Recurrent neural networks (RNN) are designed to recognize sequences, for example, a speech signal or a text. The recurrent network has cycles inside that implies the presence of short memory in the net. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "#Import MNIST\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Setting up tensforflow parameters\n",
    "learning_rate =  0.001 \n",
    "training_epochs = 300\n",
    "batch_size = 50\n",
    "\n",
    "n_inputs = 784 #MNIST data 28*28=784 for each image\n",
    "n_classes = 10 #Output can be 1 of the 10 numbers\n",
    "dropout_percentage = 0.75 #Dropout probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "##TF graph inputs\n",
    "\n",
    "#mnist images are of shape 28*28 = 784\n",
    "#We do not know the number of images/ they will be different for train and test thus None, 784\n",
    "x = tf.placeholder(tf.float32, [None, n_inputs])\n",
    "\n",
    "#y, i.e. the result can be any number from 0 to 9 in onehot encoded format\n",
    "y = tf.placeholder(tf.float32, [None, n_classes]) \n",
    "\n",
    "#Keep probaility of the possible drop outs\n",
    "keep_prob = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Defining weights and bias\n",
    "W = {## 5x5 convolution, 1 input and 32 outputs\n",
    "     \"w1\": tf.Variable(tf.random_normal([5, 5, 1, 32])),\n",
    "     ## 5x5 convolution, 32 inputs, 64 outputs\n",
    "     \"w2\": tf.Variable(tf.random_normal([5, 5, 32, 64])),\n",
    "     ## Fully connected, 7*7*64 inputs, 1024 outputs\n",
    "     \"w_full\": tf.Variable(tf.random_normal([7*7*64, 1024])),\n",
    "     ## 1024 inputs, 10 output classes\n",
    "     \"w_\": tf.Variable(tf.random_normal([1024, n_classes]))\n",
    "    \n",
    "    }\n",
    "\n",
    "b = {\"b1\": tf.Variable(tf.random_normal([32])),\n",
    "     \"b2\": tf.Variable(tf.random_normal([64])),\n",
    "     \"b_full\": tf.Variable(tf.random_normal([1024])),\n",
    "     \"b_\": tf.Variable(tf.random_normal([n_classes]))\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def maxpool2d(x, k = 2):\n",
    "    #MaxPool2D wrapper\n",
    "    return tf.nn.max_pool(x, ksize = [1, k, k, 1], \n",
    "                          strides = [1, k, k, 1], \n",
    "                          padding= \"SAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Model construction\n",
    "def conv_net(x, ws, bs, dropouts):\n",
    "    #Reshape input\n",
    "    xs = tf.reshape(x, shape = [-1, 28, 28, 1])\n",
    "    stride = 1\n",
    "    \n",
    "    #convolution layer\n",
    "    x = tf.nn.conv2d(xs, ws[\"w1\"], strides = [1, stride, stride, 1], padding = \"SAME\")\n",
    "    x = tf.nn.bias_add(x, bs[\"b1\"])\n",
    "    conv1 = tf.nn.relu(x)\n",
    "    #max pooling (down-sampling)\n",
    "    output_layer_1 = maxpool2d(conv1)\n",
    "    \n",
    "    #convolution layer\n",
    "    output_layer_2 = tf.nn.conv2d(output_layer_1, ws[\"w2\"], strides = [1, stride, stride, 1], padding = \"SAME\")\n",
    "    output_layer_2 = tf.nn.bias_add(output_layer_2, bs[\"b2\"])\n",
    "    conv2 = tf.nn.relu(output_layer_2)\n",
    "    #max pooling (down-sampling)\n",
    "    output_layer_2 = maxpool2d(conv2)\n",
    "    \n",
    "    #fully connected layer\n",
    "    #reshape output_layer_2 to fit as i/p to fully connected layer\n",
    "    full_input = tf.reshape(output_layer_2, [-1, ws[\"w_full\"].get_shape().as_list()[0]])\n",
    "    \n",
    "    full_layer_op = tf.add(tf.matmul(full_input,ws[\"w_full\"]), bs[\"b_full\"])\n",
    "    output_layer = tf.nn.relu(full_layer_op)\n",
    "    \n",
    "    #incorporate dropouts\n",
    "    out_dropout = tf.nn.dropout(output_layer, dropouts)\n",
    "    \n",
    "    output = tf.add(tf.matmul(out_dropout, ws[\"w_\"]), bs[\"b_\"])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Model call\n",
    "pred = conv_net(x, W, b, keep_prob)\n",
    "\n",
    "#Now, every time this model is rerun to train, it will work towards reducing the loss and error\n",
    "#Loss reduction will be based on cross entropy 1/(1+e**-x)\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits= pred, labels= y))\n",
    "\n",
    "# AdamOptimizer training\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "#Evaluation\n",
    "correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "##Initialize all variables and launch session\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    #Training steps\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        \n",
    "        #Training will be in batches now\n",
    "        for i in range(total_batch):\n",
    "            #pick up one batch at a time\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            _, c = sess.run([optimizer, cost], feed_dict = {x: batch_xs,\n",
    "                                                  y: batch_ys})\n",
    "            new_cost = c/total_batch\n",
    "            #computing intermediary loss\n",
    "            avg_cost = avg_cost + new_cost\n",
    "        \n",
    "        if epoch%5 ==0:\n",
    "            print \"Epoch:\", \"%02d\" % (epoch+1), \"Cost:\", avg_cost\n",
    "            \n",
    "    print \"Training finished\"  \n",
    "    \n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    # Calculate accuracy for 3000 examples\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print \"Accuracy:\", accuracy.eval({x: mnist.test.images, y: mnist.test.labels})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This 2 hidden layer model performs better than the regular logistic regression."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
